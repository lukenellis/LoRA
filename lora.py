# -*- coding: utf-8 -*-
"""LoRA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1naBAgCSmTvd8Nd_9S7bLAQXevVXdYo_5

Fine Tuning RoBERTa Base on the CoLA Task using
LoRA
"""

# Commented out IPython magic to ensure Python compatibility.
#SET UP
!git clone https://github.com/lukenellis/LoRA.git

# %cd LoRA

!pip install -e .
!pip install --upgrade transformers datasets evaluate wandb

# PARAMETERS FROM PAPER
!git pull

import os
os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":16:8"
os.environ["PYTHONHASHSEED"] = "0"


!python -m torch.distributed.launch --nproc_per_node=1 \
examples/NLU/examples/text-classification/run_glue.py \
--model_name_or_path roberta-base \
--task_name cola \
--do_train \
--do_eval \
--max_seq_length 512 \
--per_device_train_batch_size 32 \
--learning_rate 4e-4 \
--num_train_epochs 80 \
--output_dir ./cola/paper_model \
--logging_steps 10 \
--logging_dir ./cola/paper_log \
--evaluation_strategy epoch \
--save_strategy no \
--warmup_ratio 0.06 \
--apply_lora \
--lora_r 8 \
--lora_alpha 8 \
--seed 0 \
--weight_decay 0.1

# OUR MODEL
!git pull

import os
os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":16:8"
os.environ["PYTHONHASHSEED"] = "0"

!python -m torch.distributed.launch --nproc_per_node=1 \
examples/NLU/examples/text-classification/run_glue.py \
--model_name_or_path roberta-base \
--task_name cola \
--do_train \
--do_eval \
--max_seq_length 512 \
--per_device_train_batch_size 16 \
--learning_rate 2e-5 \
--num_train_epochs 4 \
--output_dir ./cola/model \
--overwrite_output_dir \
--logging_steps 100 \
--logging_dir ./cola/log \
--eval_strategy epoch \
--save_strategy no \
--apply_lora \
--lora_r 8 \
--lora_alpha 16

# OUR MODEL (dynamic rank)
!git pull

import os
os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":16:8"
os.environ["PYTHONHASHSEED"] = "0"

!torchrun --nproc_per_node=1 \
examples/NLU/examples/text-classification/run_glue.py \
--model_name_or_path roberta-base \
--task_name cola \
--do_train \
--do_eval \
--max_seq_length 128 \
--per_device_train_batch_size 12 \
--learning_rate 2e-5 \
--num_train_epochs 3 \
--output_dir ./cola/model \
--overwrite_output_dir \
--logging_steps 100 \
--logging_dir ./cola/log \
--eval_strategy epoch \
--save_strategy no \
--apply_lora \
--lora_r -1 \
--lora_alpha 16